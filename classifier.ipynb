{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shujaat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = {\"clothing\":0, \"camera\":1, \"home\":2}\n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR_DATA = 'glove.6B.100d.txt'\n",
    "\n",
    "file = open(GLOVE_DIR_DATA, encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure there are no null values in the datasets\n",
      "Has null values:  False\n",
      "Has null values:  False\n",
      "Has null values:  False\n"
     ]
    }
   ],
   "source": [
    "clothing = pd.read_csv('dataset/clothing.tsv', sep='\\t')\n",
    "cameras = pd.read_csv('dataset/cameras.tsv', sep='\\t')\n",
    "home = pd.read_csv('dataset/home.tsv', sep='\\t')\n",
    "\n",
    "dataset = [clothing, cameras, home]\n",
    "\n",
    "print(\"Make sure there are no null values in the datasets\")\n",
    "for data in dataset:\n",
    "    print('Has null values: ', data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text= text.strip().lower().split()\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    data['title'] = data['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = clothing['title'] + ' ' + cameras['title'] + ' ' + home['title']\n",
    "all_texts = all_texts.drop_duplicates(keep=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "clothing_sequences = tokenizer.texts_to_sequences(clothing['title'])\n",
    "electronics_sequences = tokenizer.texts_to_sequences(cameras['title'])\n",
    "home_appliances_sequences = tokenizer.texts_to_sequences(home['title'])\n",
    "\n",
    "clothing_data = pad_sequences(clothing_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "electronics_data = pad_sequences(electronics_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "home_appliances_data = pad_sequences(home_appliances_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tid\n",
      "--------------------\n",
      "sports\t\t16\n",
      "action\t\t13\n",
      "spy\t\t7\n",
      "pen\t\t57\n",
      "camera\t\t2\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "test_string = \"sports action spy pen camera\"\n",
    "print(\"word\\t\\tid\")\n",
    "print(\"-\" * 20)\n",
    "for word in test_string.split():\n",
    "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Vector [[16, 13, 2], [7, 57, 2]]\n",
      "Padded Vector [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 16 13  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  7 57  2]]\n"
     ]
    }
   ],
   "source": [
    "test_sequence = tokenizer.texts_to_sequences([\"sports action camera\", \"spy pen camera\"])\n",
    "padded_sequence = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"Text to Vector\", test_sequence)\n",
    "print(\"Padded Vector\", padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing: \t\t [ 1.  0.  0.]\n",
      "camera: \t\t [ 0.  1.  0.]\n",
      "home: \t\t\t [ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"clothing: \\t\\t\", to_categorical(category_index[\"clothing\"], 3))\n",
    "print(\"camera: \\t\\t\", to_categorical(category_index[\"camera\"], 3))\n",
    "print(\"home: \\t\\t\\t\", to_categorical(category_index[\"home\"], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing shape:  (392721, 100)\n",
      "electronics shape:  (1347, 100)\n",
      "home appliances shape:  (11425, 2)\n",
      "----------\n",
      "combined data shape:  (405493, 100)\n",
      "combined category/label shape:  (405493, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"clothing shape: \", clothing_data.shape)\n",
    "print(\"electronics shape: \", electronics_data.shape)\n",
    "print(\"home appliances shape: \", home.shape)\n",
    "\n",
    "data = np.vstack((clothing_data, electronics_data, home_appliances_data))\n",
    "category = pd.concat([clothing['category'], cameras['category'], home['category']]).values\n",
    "category = to_categorical(category)\n",
    "print(\"-\"*10)\n",
    "print(\"combined data shape: \", data.shape)\n",
    "print(\"combined category/label shape: \", category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "indices = np.arange(data.shape[0]) # get sequence of row index\n",
    "np.random.shuffle(indices) # shuffle the row indexes\n",
    "data = data[indices] # shuffle data/product-titles/x-axis\n",
    "category = category[indices] # shuffle labels/category/y-axis\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = category[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = category[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 2625 unique tokens.\n",
      "Null word embeddings: 973\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(150,activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "# score = model.evaluate(x_val, y_val, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          262600    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 250)           75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 753       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 401,353\n",
      "Trainable params: 138,753\n",
      "Non-trainable params: 262,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
    "model_1.add(GlobalMaxPooling1D())\n",
    "model_1.add(Dense(250))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dense(3))\n",
    "model_1.add(Activation('sigmoid'))\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283846 samples, validate on 121647 samples\n",
      "Epoch 1/2\n",
      "283846/283846 [==============================] - 943s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 2/2\n",
      "283846/283846 [==============================] - 1069s 4ms/step - loss: 5.0153e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Test loss: \t 0.00203745497136\n",
      "Test Accuracy: \t 0.999819148849\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Training the model and saving the best one!!\n",
    "batch_size = 128\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "modelsave = ModelCheckpoint(\n",
    "    filepath='product_model_1.h5', save_best_only=True, verbose=0)\n",
    "model_1.fit(\n",
    "    x_train, y_train, batch_size=batch_size,\n",
    "    epochs=2, \n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[annealer, earlystop, modelsave]\n",
    ")\n",
    "\n",
    "score = model_1.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss: \\t', score[0])\n",
    "print('Test Accuracy: \\t', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_1\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_1 = load_model('product_model_1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Predicted category:  camera\n",
      "----------\n",
      "Clothing Probability:  1.9271e-15\n",
      "Camera Probability:  0.999904\n",
      "home probability:  3.82342e-08\n"
     ]
    }
   ],
   "source": [
    "example_product = \"Nikon Coolpix A10 Point and Shoot Camera (Black)\"\n",
    "example_product = preprocess(example_product)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_product])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"-\"*10)\n",
    "print(\"Predicted category: \", category_reverse_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
    "print(\"-\"*10)\n",
    "probabilities = model_1.predict(example_padded_sequence, verbose=0)\n",
    "probabilities = probabilities[0]\n",
    "print(\"Clothing Probability: \",probabilities[category_index[\"clothing\"]] )\n",
    "print(\"Camera Probability: \",probabilities[category_index[\"camera\"]] )\n",
    "print(\"home probability: \",probabilities[category_index[\"home\"]] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
